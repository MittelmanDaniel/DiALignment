{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-26T22:19:11.448291Z",
     "start_time": "2024-10-26T22:19:05.803591Z"
    }
   },
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "\n",
    "from prompt_gen import generate_prompts"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\anaconda3\\envs\\dialignment\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:19:13.159791Z",
     "start_time": "2024-10-26T22:19:13.143763Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM",
   "id": "7d985625279bccd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:20:44.919507Z",
     "start_time": "2024-10-26T22:20:44.903697Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = \"cuda\"",
   "id": "c49655b1fbbf3135",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:21:45.911878Z",
     "start_time": "2024-10-26T22:21:45.895871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_name: str) -> HookedTransformer:\n",
    "    model = HookedTransformer.from_pretrained_no_processing(\n",
    "      model_name,\n",
    "      device=DEVICE,\n",
    "      dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    model.tokenizer.padding_side = 'left'\n",
    "    return model"
   ],
   "id": "575181eb0b857e73",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:21:48.434042Z",
     "start_time": "2024-10-26T22:21:48.418362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_harmful_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "    instructions = dataset['goal'].tolist()\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    hf_path = 'tatsu-lab/alpaca'\n",
    "    dataset = load_dataset(hf_path)\n",
    "\n",
    "    # filter for instructions that do not have inputs\n",
    "    instructions = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if dataset['train'][i]['input'].strip() == '':\n",
    "            instructions.append(dataset['train'][i]['instruction'])\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test"
   ],
   "id": "a73fa97f54f976fd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:22:56.272519Z",
     "start_time": "2024-10-26T22:22:56.256892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(model, instructions):\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": instructions}\n",
    "    ]\n",
    "    tokens = model.tokenizer.apply_chat_template(chat, tokenize=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokens"
   ],
   "id": "9794d948ab7dc0ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:23:00.908515Z",
     "start_time": "2024-10-26T22:23:00.876707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 4,\n",
    ") -> List[str]:\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "        generations.extend(generation)\n",
    "\n",
    "    return generations"
   ],
   "id": "f5e9adcd13aaf4c8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:23:46.612916Z",
     "start_time": "2024-10-26T22:23:46.581733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_mean_activation(model, tokenizer, prompts, batch_size):\n",
    "    n_samples = len(prompts)\n",
    "    n_layers = model.cfg.n_layers\n",
    "    pos = -1\n",
    "    \n",
    "    mean_act = torch.zeros((n_layers, 1, model.cfg.d_model), dtype=model.cfg.dtype, device=DEVICE)\n",
    "        \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        toks = tokenizer(instructions=prompts[i:i+batch_size])\n",
    "        logits, cache = model.run_with_cache(toks, names_filter=lambda hook_name: 'resid' in hook_name)\n",
    "        \n",
    "        for layer_i in range(n_layers):\n",
    "            mean_act[layer_i] += cache['resid_pre', layer_i][:, pos, :].sum(dim=0) / n_samples\n",
    "            \n",
    "        # memory management\n",
    "        del cache, logits\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    return mean_act"
   ],
   "id": "71734f0b1c606ef0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:24:08.920002Z",
     "start_time": "2024-10-26T22:24:08.903803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_activation_dir(desired_act, harmless_act):\n",
    "    delta_dir = desired_act - harmless_act\n",
    "    return delta_dir / delta_dir.norm(keepdim=True, dim=2)"
   ],
   "id": "912c9dc635c1f11c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:24:09.849160Z",
     "start_time": "2024-10-26T22:24:09.833531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def direction_ablation_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"]\n",
    "):\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation - proj"
   ],
   "id": "6ff18afecf81925c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:24:37.227060Z",
     "start_time": "2024-10-26T22:24:37.195526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def direction_amplify_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"]\n",
    "):\n",
    "    #proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation + direction#proj"
   ],
   "id": "e79650941f51ac90",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T22:34:31.329450Z",
     "start_time": "2024-10-26T22:34:31.297984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(model, tokenizer, hook, intervention_direction, baseline_prompts, direction_layer=None):\n",
    "    intervention_layers = list(range(1, model.cfg.n_layers))\n",
    "    if direction_layer is None:\n",
    "        fwd_hooks = [(utils.get_act_name(act_name, l), functools.partial(hook, direction=intervention_direction[l])) for l in intervention_layers for act_name in ['resid_pre']]\n",
    "    else:\n",
    "        fwd_hooks = [(utils.get_act_name(act_name, l), functools.partial(hook, direction=intervention_direction[direction_layer])) for l in intervention_layers for act_name in ['resid_pre']]\n",
    "\n",
    "    intervention_generations = get_generations(model, baseline_prompts, tokenizer, fwd_hooks=fwd_hooks)\n",
    "    baseline_generations = get_generations(model, baseline_prompts, tokenizer, fwd_hooks=[])\n",
    "    \n",
    "    for i in range(len(baseline_prompts)):\n",
    "        print(f\"INSTRUCTION {i}: {repr(baseline_prompts[i])}\")\n",
    "        print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "        print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "        print(Fore.RED + f\"INTERVENTION COMPLETION:\")\n",
    "        print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "        print(Fore.RESET)\n",
    "        \n",
    "    return intervention_generations, baseline_generations\n"
   ],
   "id": "e40d749d5d90e126",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-26T22:50:56.019997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = load_model(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer = functools.partial(tokenize, tokenizer=model.tokenizer)\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "\n",
    "n_samples = 64\n",
    "batch_size = 32\n",
    "n_inst_test = 8\n",
    "harmful_mean_act = sample_mean_activation(model, tokenizer, harmful_inst_train[:n_samples], batch_size)\n",
    "harmless_mean_act = sample_mean_activation(model, tokenizer, harmless_inst_train[:n_samples], batch_size)\n",
    "refusal_dir_bylayer = find_activation_dir(harmful_mean_act, harmless_mean_act)\n",
    "intervention_gens, baseline_gens = run_experiment(model, tokenizer, direction_ablation_hook, harmful_inst_test[:n_inst_test], direction_layer=None)"
   ],
   "id": "a75c36ab76746d95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73061eb95f903c9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
